import os
import base64
import cv2
import torch
import numpy as np
import streamlit as st
from ultralytics import YOLO
from torchvision import transforms
from PIL import Image


class VehicleDetectionSystem:
    def __init__(
            self,
            detector_path="runs/detect/vehicle_detection22/weights/best.pt",
            classifier_path="models/best_vehicle_classifier.pth",
            class_mapping_path="models/classifier_class_mapping.txt",
            conf_threshold=0.25
    ):
        st.text("åˆå§‹åŒ–è½¦è¾†æ£€æµ‹ä¸åˆ†ç±»ç³»ç»Ÿ...")

        # ä¿å­˜è·¯å¾„å’Œå‚æ•°
        self.detector_path = detector_path
        self.classifier_path = classifier_path
        self.class_mapping_path = class_mapping_path
        self.conf_threshold = conf_threshold

        # åŠ è½½æ£€æµ‹å™¨
        st.text("åŠ è½½è½¦è¾†æ£€æµ‹å™¨...")
        self.detector = YOLO(detector_path)

        # åŠ è½½ç±»åˆ«æ˜ å°„
        st.text("åŠ è½½ç±»åˆ«æ˜ å°„...")
        self.class_mapping = self.load_class_mapping()

        # åŠ è½½åˆ†ç±»å™¨
        st.text("åŠ è½½è½¦è¾†åˆ†ç±»å™¨...")
        num_classes = len(self.class_mapping)
        self.classifier, self.device = self.load_classifier(num_classes)

        # å®šä¹‰å˜æ¢
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

        st.text("ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")

    def load_classifier(self, num_classes):
        """åŠ è½½åˆ†ç±»å™¨æ¨¡å‹"""
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        st.text(f"ä½¿ç”¨è®¾å¤‡: {device}")

        model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=False)

        # ä¿®æ”¹æœ€åä¸€å±‚ä»¥åŒ¹é…ç±»åˆ«æ•°é‡
        in_features = model.fc.in_features
        model.fc = torch.nn.Linear(in_features, num_classes)

        # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
        model.load_state_dict(torch.load(self.classifier_path, map_location=device))
        model = model.to(device)
        model.eval()

        return model, device

    def load_class_mapping(self):
        """åŠ è½½ç±»åˆ«æ˜ å°„"""
        class_mapping = {}
        with open(self.class_mapping_path, 'r') as f:
            for line in f:
                parts = line.strip().split(': ')
                if len(parts) == 2:
                    idx, class_name = parts
                    class_mapping[int(idx)] = class_name

        return class_mapping

    def classify_vehicle(self, crop_img):
        """å¯¹è½¦è¾†è£å‰ªå›¾åƒè¿›è¡Œåˆ†ç±»"""
        # å°†OpenCVå›¾åƒè½¬æ¢ä¸ºPILå›¾åƒ
        if isinstance(crop_img, np.ndarray):
            crop_img = cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB)
            crop_img = Image.fromarray(crop_img)

        # åº”ç”¨å˜æ¢
        input_tensor = self.transform(crop_img).unsqueeze(0).to(self.device)

        # è¿›è¡Œé¢„æµ‹
        with torch.no_grad():
            output = self.classifier(input_tensor)
            probabilities = torch.nn.functional.softmax(output, dim=1)[0]
            _, predicted = torch.max(output, 1)

        class_idx = predicted.item()
        confidence = probabilities[class_idx].item()
        class_name = self.class_mapping.get(class_idx, f"æœªçŸ¥ç±»åˆ«({class_idx})")

        return class_name, confidence

    def process_image(self, image):
        """å¤„ç†å›¾åƒï¼Œæ£€æµ‹å¹¶åˆ†ç±»è½¦è¾†"""
        if image is None:
            st.error("è¯·å…ˆä¸Šä¼ å›¾åƒ")
            return None, "æœªæ£€æµ‹åˆ°å›¾åƒ"

        # è½¬æ¢ä¸ºOpenCVæ ¼å¼
        if isinstance(image, np.ndarray) and len(image.shape) == 3 and image.shape[2] == 4:
            # å¦‚æœå›¾åƒæ˜¯RGBAæ ¼å¼ï¼Œè½¬æ¢ä¸ºRGB
            image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)

        # ä¿å­˜ä¸´æ—¶å›¾åƒ
        temp_path = "temp_image.jpg"
        cv2.imwrite(temp_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))

        # è¯»å–å›¾åƒ
        img = cv2.imread(temp_path)
        if img is None:
            st.error("æ— æ³•è¯»å–å›¾åƒ")
            return None, "æ— æ³•è¯»å–å›¾åƒ"

        # è¿è¡Œæ£€æµ‹
        results = self.detector(img, conf=self.conf_threshold)

        # åˆ›å»ºç»“æœå›¾åƒçš„å‰¯æœ¬
        result_img = img.copy()

        # å­˜å‚¨æ£€æµ‹ç»“æœ
        detections = []

        # å¤„ç†æ¯ä¸ªæ£€æµ‹ç»“æœ
        for i, det in enumerate(results[0].boxes):
            x1, y1, x2, y2 = map(int, det.xyxy[0].cpu().numpy())
            conf = float(det.conf[0].cpu().numpy())

            # è£å‰ªè½¦è¾†å›¾åƒ
            crop_img = img[y1:y2, x1:x2]
            if crop_img.size == 0:
                continue

            # åˆ†ç±»è½¦è¾†
            class_name, class_conf = self.classify_vehicle(crop_img)

            # ç»˜åˆ¶æ£€æµ‹æ¡†
            cv2.rectangle(result_img, (x1, y1), (x2, y2), (0, 255, 0), 2)

            # ç»˜åˆ¶æ ‡ç­¾
            label = f"{class_name} {conf:.2f}"
            t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)[0]
            cv2.rectangle(result_img, (x1, y1 - t_size[1] - 10), (x1 + t_size[0], y1), (0, 255, 0), -1)
            cv2.putText(result_img, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)

            # ä¿å­˜æ£€æµ‹ç»“æœ
            detections.append({
                "bbox": [x1, y1, x2, y2],
                "confidence": conf,
                "class_name": class_name,
                "class_confidence": class_conf
            })

        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_path):
            os.remove(temp_path)

        # ç»Ÿè®¡æ£€æµ‹åˆ°çš„è½¦è¾†ç±»å‹
        vehicle_counts = {}
        for det in detections:
            vehicle_type = det["class_name"]
            if vehicle_type in vehicle_counts:
                vehicle_counts[vehicle_type] += 1
            else:
                vehicle_counts[vehicle_type] = 1

        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        stats = f"æ£€æµ‹åˆ° {len(detections)} è¾†è½¦:\n"
        for vehicle_type, count in vehicle_counts.items():
            stats += f"- {vehicle_type}: {count} è¾†\n"

        # è½¬æ¢ä¸ºRGBä»¥ä¾¿Streamlitæ˜¾ç¤º
        result_img = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)

        return result_img, stats

    def process_video(self, video_file):
        """å¤„ç†è§†é¢‘ï¼Œæ£€æµ‹å¹¶åˆ†ç±»è½¦è¾†"""
        if video_file is None:
            st.error("è¯·å…ˆä¸Šä¼ è§†é¢‘")
            return None, None
    
        # ä¿å­˜ä¸´æ—¶è§†é¢‘
        temp_path = "temp_video.mp4"
        with open(temp_path, "wb") as f:
            f.write(video_file.read())
    
        output_path = os.path.join(os.getcwd(), "results", "processed_video.mp4")
        print(f"è¾“å‡ºè·¯å¾„: {output_path}")  # æ·»åŠ æ‰“å°è¯­å¥æŸ¥çœ‹è·¯å¾„
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
        # æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(temp_path)
        if not cap.isOpened():
            st.error("æ— æ³•æ‰“å¼€è§†é¢‘")
            return None, None
    
        # è·å–è§†é¢‘ä¿¡æ¯
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
        st.text(f"è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps} FPS, {total_frames} å¸§")
    
        # åˆ›å»ºè¾“å‡ºè§†é¢‘
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))
        if not out.isOpened():
            st.error("æ— æ³•åˆ›å»ºè¾“å‡ºè§†é¢‘æ–‡ä»¶")
            cap.release()
            return None, None
    
        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = st.progress(0)
        status_text = st.empty()
    
        frame_count = 0
        vehicle_counts = {}  # ç”¨äºç»Ÿè®¡è½¦è¾†ç±»å‹
    
        # å¤„ç†æ¯ä¸€å¸§
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break
    
            frame_count += 1
            progress = frame_count / total_frames
            progress_bar.progress(progress)
            status_text.text(f"å¤„ç†å¸§ {frame_count}/{total_frames} ({progress * 100:.1f}%)")
    
            # è¿è¡Œæ£€æµ‹
            results = self.detector(frame, conf=self.conf_threshold)
    
            # åˆ›å»ºç»“æœå¸§çš„å‰¯æœ¬
            result_frame = frame.copy()
    
            # å¤„ç†æ¯ä¸ªæ£€æµ‹ç»“æœ
            for i, det in enumerate(results[0].boxes):
                x1, y1, x2, y2 = map(int, det.xyxy[0].cpu().numpy())
                conf = float(det.conf[0].cpu().numpy())
    
                # è£å‰ªè½¦è¾†å›¾åƒ
                crop_img = frame[y1:y2, x1:x2]
                if crop_img.size == 0:
                    continue
    
                # åˆ†ç±»è½¦è¾†
                class_name, class_conf = self.classify_vehicle(crop_img)
    
                # æ›´æ–°è½¦è¾†ç»Ÿè®¡
                if class_name in vehicle_counts:
                    vehicle_counts[class_name] += 1
                else:
                    vehicle_counts[class_name] = 1
    
                # ç»˜åˆ¶æ£€æµ‹æ¡†
                cv2.rectangle(result_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
    
                # ç»˜åˆ¶æ ‡ç­¾
                label = f"{class_name} {conf:.2f}"
                t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)[0]
                cv2.rectangle(result_frame, (x1, y1 - t_size[1] - 10), (x1 + t_size[0], y1), (0, 255, 0), -1)
                cv2.putText(result_frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)
    
            # æ·»åŠ å¸§è®¡æ•°å™¨
            cv2.putText(result_frame, f"Frame: {frame_count}/{total_frames}", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
    
            # å†™å…¥è¾“å‡ºè§†é¢‘
            try:
                out.write(result_frame)
            except Exception as e:
                st.error(f"å†™å…¥å¸§æ—¶å‡ºé”™: {e}")
                break
    
        # é‡Šæ”¾èµ„æº
        cap.release()
        out.release()
    
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_path):
            os.remove(temp_path)
    
        status_text.text(f"å¤„ç†å®Œæˆ! å…±å¤„ç† {frame_count} å¸§")
        st.success(f"å¤„ç†åçš„è§†é¢‘å·²ä¿å­˜åˆ°: {output_path}")
    
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        total_vehicles = sum(vehicle_counts.values())
        stats = f"è§†é¢‘ä¸­æ£€æµ‹åˆ° {total_vehicles} è¾†è½¦:\n"
        for vehicle_type, count in vehicle_counts.items():
            stats += f"- {vehicle_type}: {count} è¾†\n"
    
        return output_path, stats, width, height, fps, total_frames, frame_count


def main():
    st.set_page_config(
        page_title="è½¦è¾†æ£€æµ‹ä¸åˆ†ç±»ç³»ç»Ÿ",
        page_icon="ğŸš—",
        layout="wide"
    )

    # è®¾ç½®å…¨å±€æ ·å¼
    st.markdown("""
    <style>
    .stTabs [data-baseweb="tab-list"] {
        gap: 8px;
    }
    .stTabs [data-baseweb="tab"] {
        border-radius: 4px 4px 0px 0px;
        padding: 10px 16px;
        background-color: #F8F9FA;
    }
    .stTabs [aria-selected="true"] {
        background-color: #4B8BF5 !important;
        color: white !important;
    }
    .stButton > button {
        width: 100%;
        background-color: #4B8BF5;
        color: white;
        border: none;
        padding: 10px 24px;
        border-radius: 4px;
        font-weight: 500;
    }
    .stButton > button:hover {
        background-color: #1967D2;
    }
    .empty-result {
        width: 100%;
        height: 300px;
        border: 1px dashed #E0E0E0;
        border-radius: 10px;
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        background-color: #F8F9FA;
    }
    .empty-icon {
        font-size: 48px;
        color: #80868B;
        margin-bottom: 16px;
    }
    .empty-text {
        color: #5F6368;
        font-size: 16px;
        text-align: center;
    }
    .stats-card {
        background-color: #F8F9FA;
        border-radius: 8px;
        padding: 16px;
        border-left: 4px solid #4B8BF5;
        margin-bottom: 20px;
    }
    </style>
    """, unsafe_allow_html=True)

    st.title("è½¦è¾†æ£€æµ‹ä¸åˆ†ç±»ç³»ç»Ÿ")
    st.markdown("ä¸Šä¼ å›¾åƒæˆ–è§†é¢‘ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨æ£€æµ‹å’Œåˆ†ç±»è½¦è¾†")

    # åˆå§‹åŒ–ç³»ç»Ÿï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶ï¼‰
    @st.cache_resource
    def load_detection_system():
        return VehicleDetectionSystem(
            detector_path="runs/detect/vehicle_detection22/weights/best.pt",
            classifier_path="models/best_vehicle_classifier.pth",
            class_mapping_path="models/classifier_class_mapping.txt",
            conf_threshold=0.25
        )

    # ä½¿ç”¨try-exceptæ•è·å¯èƒ½çš„åˆå§‹åŒ–é”™è¯¯
    try:
        system = load_detection_system()
    except Exception as e:
        st.error(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
        st.error("è¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨äºæ­£ç¡®çš„è·¯å¾„")
        st.stop()

    # åˆ›å»ºé€‰é¡¹å¡
    tab1, tab2 = st.tabs(["å›¾åƒæ£€æµ‹", "è§†é¢‘æ£€æµ‹"])

    # å›¾åƒæ£€æµ‹é€‰é¡¹å¡
    with tab1:
        st.header("å›¾åƒæ£€æµ‹")
        col1, col2 = st.columns(2)

        with col1:
            st.markdown("### è¾“å…¥å›¾åƒ")
            
            # ç›´æ¥ä½¿ç”¨Streamlitçš„ä¸Šä¼ æ§ä»¶
            uploaded_image = st.file_uploader("ä¸Šä¼ å›¾åƒ", type=["jpg", "jpeg", "png"], key="image_uploader")
            
            # æ˜¾ç¤ºæ£€æµ‹æŒ‰é’®åœ¨ä¸Šä¼ æ§ä»¶ä¸‹æ–¹
            if uploaded_image is not None:
                # æ˜¾ç¤ºæ£€æµ‹æŒ‰é’®åœ¨ä¸Šä¼ æ§ä»¶å’Œå›¾åƒä¹‹é—´
                if st.button("å¼€å§‹æ£€æµ‹", key="image_detect"):
                    with st.spinner("æ­£åœ¨å¤„ç†å›¾åƒ..."):
                        result_img, stats = system.process_image(np.array(Image.open(uploaded_image)))
                        st.session_state.result_img = result_img
                        st.session_state.stats = stats
            
            # ä»…å½“ä¸Šä¼ äº†å›¾åƒæ—¶æ‰æ˜¾ç¤ºå›¾åƒ
            if uploaded_image is not None:
                image = Image.open(uploaded_image)
                st.image(image, caption="ä¸Šä¼ çš„å›¾åƒ", use_column_width=True)

        with col2:
            st.markdown("### æ£€æµ‹ç»“æœ")
            if 'result_img' in st.session_state and st.session_state.result_img is not None:
                # åˆ›å»ºç»“æœå®¹å™¨
                result_container = st.container()
                with result_container:
                    # å…ˆæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                    stats_html = f"""
                    <div class="stats-card">
                        <h4>ç»Ÿè®¡ä¿¡æ¯</h4>
                        <pre>{st.session_state.stats.replace(chr(10), '<br>')}</pre>
                    </div>
                    """
                    st.markdown(stats_html, unsafe_allow_html=True)
                    
                    # å†æ˜¾ç¤ºæ£€æµ‹ç»“æœå›¾åƒ
                    st.image(st.session_state.result_img, caption="æ£€æµ‹ç»“æœ", use_column_width=True)
            else:
                # åˆ›å»ºç©ºç»“æœåŒºåŸŸ
                st.markdown("""
                <div class="empty-result">
                    <div class="empty-icon">ğŸ”</div>
                    <div class="empty-text">è¯·ä¸Šä¼ å›¾åƒå¹¶ç‚¹å‡»å¼€å§‹æ£€æµ‹æŸ¥çœ‹ç»“æœ</div>
                </div>
                """, unsafe_allow_html=True)

    # è§†é¢‘æ£€æµ‹é€‰é¡¹å¡
    with tab2:
        st.header("è§†é¢‘æ£€æµ‹")
        col1, col2 = st.columns(2)

        with col1:
            st.markdown("### ä¸Šä¼ è§†é¢‘")
            uploaded_video = st.file_uploader("ç‚¹å‡»æˆ–æ‹–æ‹½è§†é¢‘åˆ°æ­¤å¤„", type=["mp4", "avi", "mov"], label_visibility='hidden')
            if uploaded_video is not None:
                st.video(uploaded_video)

        with col2:
            st.markdown("### å¤„ç†ç»“æœ")
            if uploaded_video is not None:
                if st.button("å¼€å§‹æ£€æµ‹", key="video_detect"):
                    with st.spinner("æ­£åœ¨å¤„ç†è§†é¢‘..."):
                        output_path, stats, width, height, fps, total_frames, frame_count = system.process_video(uploaded_video)
                        st.session_state.output_path = output_path
                        st.session_state.video_stats = stats
                        if output_path and os.path.exists(output_path):
                            st.success("è§†é¢‘å¤„ç†å®Œæˆ!")
                            st.text(f"è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps} FPS, {total_frames} å¸§")
                            st.text(f"å¤„ç†å®Œæˆ! å…±å¤„ç† {frame_count} å¸§")
                            st.text(f"å¤„ç†åçš„è§†é¢‘å·²ä¿å­˜åˆ°: {output_path}")
                        else:
                            st.error("è§†é¢‘å¤„ç†å¤±è´¥")

            if 'output_path' in st.session_state and st.session_state.output_path and os.path.exists(st.session_state.output_path):
                try:
                    with open(st.session_state.output_path, "rb") as f:
                        video_bytes = f.read()
                        video_base64 = base64.b64encode(video_bytes).decode('utf-8')
                    video_html = f"""
                    <video width="100%" controls>
                        <source src="data:video/mp4;base64,{video_base64}" type="video/mp4">
                        æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè§†é¢‘æ ‡ç­¾ã€‚
                    </video>
                    """
                    st.markdown(video_html, unsafe_allow_html=True)
                except Exception as e:
                    st.error(f"å±•ç¤ºè§†é¢‘æ—¶å‡ºé”™: {e}")
            else:
                st.info("è¯·ä¸Šä¼ è§†é¢‘å¹¶ç‚¹å‡»å¼€å§‹æ£€æµ‹æŸ¥çœ‹ç»“æœ")

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            if 'video_stats' in st.session_state:
                stats_html = f"""
                <div class="stats-card">
                    <h4>ç»Ÿè®¡ä¿¡æ¯</h4>
                    <pre>{st.session_state.video_stats.replace(chr(10), '<br>')}</pre>
                </div>
                """
                st.markdown(stats_html, unsafe_allow_html=True)

        # è§†é¢‘å¤„ç†è¯´æ˜
        with st.expander("è§†é¢‘å¤„ç†è¯´æ˜"):
            st.markdown("""
            - è§†é¢‘å¤„ç†å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…
            - å¤„ç†åçš„è§†é¢‘å°†æ˜¾ç¤ºæ£€æµ‹æ¡†å’Œåˆ†ç±»ç»“æœ
            - å¤„ç†é€Ÿåº¦å–å†³äºè§†é¢‘é•¿åº¦å’Œåˆ†è¾¨ç‡
            - å¤„ç†åçš„è§†é¢‘å°†ä¿å­˜åœ¨resultsç›®å½•ä¸‹
            """)


if __name__ == "__main__":
    main()