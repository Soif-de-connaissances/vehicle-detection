import os
import cv2
import torch
import numpy as np
import streamlit as st
from ultralytics import YOLO
from torchvision import transforms
from PIL import Image


class VehicleDetectionSystem:
    def __init__(
            self,
            detector_path="runs/detect/vehicle_detection22/weights/best.pt",
            classifier_path="models/best_vehicle_classifier.pth",
            class_mapping_path="models/classifier_class_mapping.txt",
            conf_threshold=0.25
    ):
        st.text("åˆå§‹åŒ–è½¦è¾†æ£€æµ‹ä¸åˆ†ç±»ç³»ç»Ÿ...")

        # ä¿å­˜è·¯å¾„å’Œå‚æ•°
        self.detector_path = detector_path
        self.classifier_path = classifier_path
        self.class_mapping_path = class_mapping_path
        self.conf_threshold = conf_threshold

        # åŠ è½½æ£€æµ‹å™¨
        st.text("åŠ è½½è½¦è¾†æ£€æµ‹å™¨...")
        self.detector = YOLO(detector_path)

        # åŠ è½½ç±»åˆ«æ˜ å°„
        st.text("åŠ è½½ç±»åˆ«æ˜ å°„...")
        self.class_mapping = self.load_class_mapping()

        # åŠ è½½åˆ†ç±»å™¨
        st.text("åŠ è½½è½¦è¾†åˆ†ç±»å™¨...")
        num_classes = len(self.class_mapping)
        self.classifier, self.device = self.load_classifier(num_classes)

        # å®šä¹‰å˜æ¢
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

        st.text("ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")

    def load_classifier(self, num_classes):
        """åŠ è½½åˆ†ç±»å™¨æ¨¡å‹"""
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        st.text(f"ä½¿ç”¨è®¾å¤‡: {device}")

        model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=False)

        # ä¿®æ”¹æœ€åä¸€å±‚ä»¥åŒ¹é…ç±»åˆ«æ•°é‡
        in_features = model.fc.in_features
        model.fc = torch.nn.Linear(in_features, num_classes)

        # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
        model.load_state_dict(torch.load(self.classifier_path, map_location=device))
        model = model.to(device)
        model.eval()

        return model, device

    def load_class_mapping(self):
        """åŠ è½½ç±»åˆ«æ˜ å°„"""
        class_mapping = {}
        with open(self.class_mapping_path, 'r') as f:
            for line in f:
                parts = line.strip().split(': ')
                if len(parts) == 2:
                    idx, class_name = parts
                    class_mapping[int(idx)] = class_name

        return class_mapping

    def classify_vehicle(self, crop_img):
        """å¯¹è½¦è¾†è£å‰ªå›¾åƒè¿›è¡Œåˆ†ç±»"""
        # å°†OpenCVå›¾åƒè½¬æ¢ä¸ºPILå›¾åƒ
        if isinstance(crop_img, np.ndarray):
            crop_img = cv2.cvtColor(crop_img, cv2.COLOR_BGR2RGB)
            crop_img = Image.fromarray(crop_img)

        # åº”ç”¨å˜æ¢
        input_tensor = self.transform(crop_img).unsqueeze(0).to(self.device)

        # è¿›è¡Œé¢„æµ‹
        with torch.no_grad():
            output = self.classifier(input_tensor)
            probabilities = torch.nn.functional.softmax(output, dim=1)[0]
            _, predicted = torch.max(output, 1)

        class_idx = predicted.item()
        confidence = probabilities[class_idx].item()
        class_name = self.class_mapping.get(class_idx, f"æœªçŸ¥ç±»åˆ«({class_idx})")

        return class_name, confidence

    def process_image(self, image):
        """å¤„ç†å›¾åƒï¼Œæ£€æµ‹å¹¶åˆ†ç±»è½¦è¾†"""
        if image is None:
            st.error("è¯·å…ˆä¸Šä¼ å›¾åƒ")
            return None, "æœªæ£€æµ‹åˆ°å›¾åƒ"

        # è½¬æ¢ä¸ºOpenCVæ ¼å¼
        if isinstance(image, np.ndarray) and len(image.shape) == 3 and image.shape[2] == 4:
            # å¦‚æœå›¾åƒæ˜¯RGBAæ ¼å¼ï¼Œè½¬æ¢ä¸ºRGB
            image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)

        # ä¿å­˜ä¸´æ—¶å›¾åƒ
        temp_path = "temp_image.jpg"
        cv2.imwrite(temp_path, cv2.cvtColor(image, cv2.COLOR_RGB2BGR))

        # è¯»å–å›¾åƒ
        img = cv2.imread(temp_path)
        if img is None:
            st.error("æ— æ³•è¯»å–å›¾åƒ")
            return None, "æ— æ³•è¯»å–å›¾åƒ"

        # è¿è¡Œæ£€æµ‹
        results = self.detector(img, conf=self.conf_threshold)

        # åˆ›å»ºç»“æœå›¾åƒçš„å‰¯æœ¬
        result_img = img.copy()

        # å­˜å‚¨æ£€æµ‹ç»“æœ
        detections = []

        # å¤„ç†æ¯ä¸ªæ£€æµ‹ç»“æœ
        for i, det in enumerate(results[0].boxes):
            x1, y1, x2, y2 = map(int, det.xyxy[0].cpu().numpy())
            conf = float(det.conf[0].cpu().numpy())

            # è£å‰ªè½¦è¾†å›¾åƒ
            crop_img = img[y1:y2, x1:x2]
            if crop_img.size == 0:
                continue

            # åˆ†ç±»è½¦è¾†
            class_name, class_conf = self.classify_vehicle(crop_img)

            # ç»˜åˆ¶æ£€æµ‹æ¡†
            cv2.rectangle(result_img, (x1, y1), (x2, y2), (0, 255, 0), 2)

            # ç»˜åˆ¶æ ‡ç­¾
            label = f"{class_name} {conf:.2f}"
            t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)[0]
            cv2.rectangle(result_img, (x1, y1 - t_size[1] - 10), (x1 + t_size[0], y1), (0, 255, 0), -1)
            cv2.putText(result_img, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)

            # ä¿å­˜æ£€æµ‹ç»“æœ
            detections.append({
                "bbox": [x1, y1, x2, y2],
                "confidence": conf,
                "class_name": class_name,
                "class_confidence": class_conf
            })

        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_path):
            os.remove(temp_path)

        # ç»Ÿè®¡æ£€æµ‹åˆ°çš„è½¦è¾†ç±»å‹
        vehicle_counts = {}
        for det in detections:
            vehicle_type = det["class_name"]
            if vehicle_type in vehicle_counts:
                vehicle_counts[vehicle_type] += 1
            else:
                vehicle_counts[vehicle_type] = 1

        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        stats = f"æ£€æµ‹åˆ° {len(detections)} è¾†è½¦:\n"
        for vehicle_type, count in vehicle_counts.items():
            stats += f"- {vehicle_type}: {count} è¾†\n"

        # è½¬æ¢ä¸ºRGBä»¥ä¾¿Streamlitæ˜¾ç¤º
        result_img = cv2.cvtColor(result_img, cv2.COLOR_BGR2RGB)

        return result_img, stats

    def process_video(self, video_file):
        """å¤„ç†è§†é¢‘ï¼Œæ£€æµ‹å¹¶åˆ†ç±»è½¦è¾†"""
        if video_file is None:
            st.error("è¯·å…ˆä¸Šä¼ è§†é¢‘")
            return None

        # ä¿å­˜ä¸´æ—¶è§†é¢‘
        temp_path = "temp_video.mp4"
        with open(temp_path, "wb") as f:
            f.write(video_file.read())

        output_path = "results/processed_video.mp4"

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs("results", exist_ok=True)

        # æ‰“å¼€è§†é¢‘
        cap = cv2.VideoCapture(temp_path)
        if not cap.isOpened():
            st.error("æ— æ³•æ‰“å¼€è§†é¢‘")
            return None

        # è·å–è§†é¢‘ä¿¡æ¯
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        st.text(f"è§†é¢‘ä¿¡æ¯: {width}x{height}, {fps} FPS, {total_frames} å¸§")

        # åˆ›å»ºè¾“å‡ºè§†é¢‘
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

        # åˆ›å»ºè¿›åº¦æ¡
        progress_bar = st.progress(0)
        status_text = st.empty()

        frame_count = 0

        # å¤„ç†æ¯ä¸€å¸§
        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            frame_count += 1
            progress = frame_count / total_frames
            progress_bar.progress(progress)
            status_text.text(f"å¤„ç†å¸§ {frame_count}/{total_frames} ({progress * 100:.1f}%)")

            # è¿è¡Œæ£€æµ‹
            results = self.detector(frame, conf=self.conf_threshold)

            # åˆ›å»ºç»“æœå¸§çš„å‰¯æœ¬
            result_frame = frame.copy()

            # å¤„ç†æ¯ä¸ªæ£€æµ‹ç»“æœ
            for i, det in enumerate(results[0].boxes):
                x1, y1, x2, y2 = map(int, det.xyxy[0].cpu().numpy())
                conf = float(det.conf[0].cpu().numpy())

                # è£å‰ªè½¦è¾†å›¾åƒ
                crop_img = frame[y1:y2, x1:x2]
                if crop_img.size == 0:
                    continue

                # åˆ†ç±»è½¦è¾†
                class_name, class_conf = self.classify_vehicle(crop_img)

                # ç»˜åˆ¶æ£€æµ‹æ¡†
                cv2.rectangle(result_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)

                # ç»˜åˆ¶æ ‡ç­¾
                label = f"{class_name} {conf:.2f}"
                t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)[0]
                cv2.rectangle(result_frame, (x1, y1 - t_size[1] - 10), (x1 + t_size[0], y1), (0, 255, 0), -1)
                cv2.putText(result_frame, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)

            # æ·»åŠ å¸§è®¡æ•°å™¨
            cv2.putText(result_frame, f"Frame: {frame_count}/{total_frames}", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

            # å†™å…¥è¾“å‡ºè§†é¢‘
            out.write(result_frame)

        # é‡Šæ”¾èµ„æº
        cap.release()
        out.release()

        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_path):
            os.remove(temp_path)

        status_text.text(f"å¤„ç†å®Œæˆ! å…±å¤„ç† {frame_count} å¸§")
        st.success(f"å¤„ç†åçš„è§†é¢‘å·²ä¿å­˜åˆ°: {output_path}")

        return output_path


def main():
    st.set_page_config(
        page_title="è½¦è¾†æ£€æµ‹ä¸åˆ†ç±»ç³»ç»Ÿ",
        page_icon="ğŸš—",
        layout="wide"
    )

    st.title("è½¦è¾†æ£€æµ‹ä¸åˆ†ç±»ç³»ç»Ÿ")
    st.markdown("ä¸Šä¼ å›¾åƒæˆ–è§†é¢‘ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨æ£€æµ‹å’Œåˆ†ç±»è½¦è¾†")

    # åˆå§‹åŒ–ç³»ç»Ÿï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶ï¼‰
    @st.cache_resource
    def load_detection_system():
        return VehicleDetectionSystem(
            detector_path="runs/detect/vehicle_detection22/weights/best.pt",
            classifier_path="models/best_vehicle_classifier.pth",
            class_mapping_path="models/classifier_class_mapping.txt",
            conf_threshold=0.25
        )

    # ä½¿ç”¨try-exceptæ•è·å¯èƒ½çš„åˆå§‹åŒ–é”™è¯¯
    try:
        system = load_detection_system()
    except Exception as e:
        st.error(f"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
        st.error("è¯·ç¡®ä¿æ¨¡å‹æ–‡ä»¶å­˜åœ¨äºæ­£ç¡®çš„è·¯å¾„")
        st.stop()

    # åˆ›å»ºé€‰é¡¹å¡
    tab1, tab2 = st.tabs(["å›¾åƒæ£€æµ‹", "è§†é¢‘æ£€æµ‹"])

    # å›¾åƒæ£€æµ‹é€‰é¡¹å¡
    with tab1:
        st.header("å›¾åƒæ£€æµ‹")

        uploaded_image = st.file_uploader("ä¸Šä¼ å›¾åƒ", type=["jpg", "jpeg", "png"])

        col1, col2 = st.columns(2)

        with col1:
            if uploaded_image is not None:
                # æ˜¾ç¤ºåŸå§‹å›¾åƒ
                image = Image.open(uploaded_image)
                st.image(image, caption="ä¸Šä¼ çš„å›¾åƒ", use_column_width=True)

                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                image_np = np.array(image)

                # æ·»åŠ æ£€æµ‹æŒ‰é’®
                if st.button("å¼€å§‹æ£€æµ‹", key="image_detect"):
                    with st.spinner("æ­£åœ¨å¤„ç†å›¾åƒ..."):
                        result_img, stats = system.process_image(image_np)

                        # ä¿å­˜ç»“æœåˆ°session state
                        st.session_state.result_img = result_img
                        st.session_state.stats = stats

        with col2:
            # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
            if 'result_img' in st.session_state and st.session_state.result_img is not None:
                st.image(st.session_state.result_img, caption="æ£€æµ‹ç»“æœ", use_column_width=True)
                st.text_area("ç»Ÿè®¡ä¿¡æ¯", st.session_state.stats, height=150)

    # è§†é¢‘æ£€æµ‹é€‰é¡¹å¡
    with tab2:
        st.header("è§†é¢‘æ£€æµ‹")

        uploaded_video = st.file_uploader("ä¸Šä¼ è§†é¢‘", type=["mp4", "avi", "mov"])

        if uploaded_video is not None:
            # æ˜¾ç¤ºä¸Šä¼ çš„è§†é¢‘
            st.video(uploaded_video)

            # æ·»åŠ æ£€æµ‹æŒ‰é’®
            if st.button("å¼€å§‹æ£€æµ‹", key="video_detect"):
                with st.spinner("æ­£åœ¨å¤„ç†è§†é¢‘..."):
                    output_path = system.process_video(uploaded_video)

                    if output_path and os.path.exists(output_path):
                        # æ˜¾ç¤ºå¤„ç†åçš„è§†é¢‘
                        st.success("è§†é¢‘å¤„ç†å®Œæˆ!")
                        st.video(output_path)
                    else:
                        st.error("è§†é¢‘å¤„ç†å¤±è´¥")

        # è§†é¢‘å¤„ç†è¯´æ˜
        with st.expander("è§†é¢‘å¤„ç†è¯´æ˜"):
            st.markdown("""
            - è§†é¢‘å¤„ç†å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…
            - å¤„ç†åçš„è§†é¢‘å°†æ˜¾ç¤ºæ£€æµ‹æ¡†å’Œåˆ†ç±»ç»“æœ
            - å¤„ç†é€Ÿåº¦å–å†³äºè§†é¢‘é•¿åº¦å’Œåˆ†è¾¨ç‡
            - å¤„ç†åçš„è§†é¢‘å°†ä¿å­˜åœ¨resultsç›®å½•ä¸‹
            """)


if __name__ == "__main__":
    main()